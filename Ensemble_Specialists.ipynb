{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Group Project -- Final Deliverables\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "\n",
    "**Paper Name: Distilling the Knowledge in a Neural Network**<br>\n",
    "**Paper URL: https://arxiv.org/abs/1503.02531**\n",
    "\n",
    "**Team name: ENSEMBLE**<br>\n",
    "**Team members:**<br>\n",
    "-- Timothy Lee (lee709@g.harvard.edu)\n",
    "\n",
    "-- Shiyun Qiu (shiyunqiu@g.harvard.edu)\n",
    "\n",
    "-- Xiangru Shu (xiangru_shu@g.harvard.edu)\n",
    "\n",
    "-- Yuyue Wang (yuyue_wang@g.harvard.edu)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large-scale machine learning tasks, such as speech and object recognition, a cumbersome model is commonly used to acheive high accuracy. The cumbersome model could be an ensemble of smaller models or a single large model trained with a strong regularizer. However, it is impractical to deploy the cumbersome model to a large number of users due to limited computational resources. One solution to this problem is to transfer the knowledge from the cumbersome model to a small model more suitable for deployment through \"distillation\".\n",
    "\n",
    "When we are distilling the knowledge of a cumbersome model into a small model, we should train the small model to generalize in the same way as the cumbersome model so that the small model can generalize well to new data. One way to transfer the generalization ability is to use the class probabilities produced by the cumbersome model as \"soft targets\" when training the small model. When the large model is an ensemble of simpler models, we can take an arithmetic or geometric mean of the predictive distributions as the soft targets. As these soft targets contain probabilities of all incorrect classes in addition to the probability of the correct class, we can obtain a lot of information about how the large model generalizes from the relative probabilites in each training case. Soft targets with high entropy have much less variance in the gradient between training cases, so we can train the small model with less data and a higher learning rate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distillation\n",
    "\n",
    "#### Class Probability\n",
    "For neural networks, the class probabilities, $q_i$ are usually computed through the softmax output layer,\n",
    "$$ q_i = \\frac{\\exp\\{z_i/T\\}}{\\Sigma_j\\exp\\{z_j/T\\}},$$\n",
    "where $z_i$'s are the logits, i.e. the vector of raw (non-normalized) predictions that a classification model generates, and T is a temperature which is normally set to 1. Higher temperature leads to similar class probabilities (similar $q_i$ values), so it produces a softer probability distribution over classes. \n",
    "\n",
    "#### Simplest Form of Distillation \n",
    "The distilled model will be trained on a transfer set. Each training case in the transfer set is a soft target distribution produced by the cumbersome model with a high temperature in its softmax. We will use the same high temperature to train the distilled model. After the model is trained, we will use a temperature of 1 to predict.\n",
    "\n",
    "#### Correct Labels Partially Known for the Transfer Set\n",
    "Two objective functions are needed.\n",
    "\n",
    "Objective function 1: cross entropy with the soft targets.<br>\n",
    "We need to use the same high temperature used to train the cumbersome model in the softmax of the distilled model. \n",
    "\n",
    "Objective function 2: cross entropy with the correct labels.<br>\n",
    "We need to use the same logits in softmax of the distilled model but at a temperature of 1.\n",
    "\n",
    "$$ \\mathcal{L}(x; W) = \\alpha * \\mathcal{H}(\\sigma(z_c/\\tau), \\sigma(z_d/\\tau)) + \\beta * \\mathcal{H}(y, \\sigma(z_d)), $$\n",
    "where x is the input, W is the model parameters in the distilled model, $\\mathcal{H}$ is the cross entropy function, $\\sigma$ is the softmax function, $\\tau$ is the temperature used to train the cumbersome model, $z_c$ is the logit of the cumbersome model, $z_d$ is the logit of the distilled model, and $\\alpha$ and $\\beta$ are the weights ($\\beta=1-\\alpha$).\n",
    "\n",
    "A weighted average of these two objective functions will be computed with a considerably lower weight placed on the second one (i.e. significantly smaller $\\beta$). \n",
    "\n",
    "The magnitudes of the graidents of soft targets scale as $1/T^2$. To prevent the relative contributions of the hard and soft targets from changing when we adjust meta-parameters, we will multiply both of them by $T^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Matching as Special Case of Distillation\n",
    "\n",
    "Previous work by Caruana and his collaborators used logits produced by the softmax as the targets for training the small model by minimizing the squared difference between the logits produced by the cumbersome model and those produced by the small model. In this section, we will show that matching the logits is indeed a special case of distillation.\n",
    "\n",
    "The cross-entropy with the soft targets for each class $i$ can be computed as the following,\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathcal{H}(\\sigma(z_{ci}/\\tau), \\sigma(z_{di}/\\tau)) \n",
    "&= - \\frac{\\exp\\{z_{ci}/\\tau\\}}{\\Sigma_j\\exp\\{z_{cj}/\\tau\\}} * \\log (\\frac{\\exp\\{z_{di}/\\tau\\}}{\\Sigma_j\\exp\\{z_{dj}/\\tau\\}})\\\\\n",
    "&= - \\frac{\\exp\\{z_{ci}/\\tau\\}}{\\Sigma_j\\exp\\{z_{cj}/\\tau\\}} * (z_{di}/\\tau - \n",
    "\\log (\\Sigma_j\\exp\\{z_{dj}/\\tau\\})).\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Taking gradient of the cross-entropy with respect to $z_{di}$ gives us,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{H}(\\sigma(z_{ci}/\\tau), \\sigma(z_{di}/\\tau))}{\\partial z_{di}}\n",
    "&= \\frac{1}{\\tau}(\\frac{\\exp\\{z_{di}/\\tau\\}}{\\Sigma_j\\exp\\{z_{dj}/\\tau\\}} - \\frac{\\exp\\{z_{ci}/\\tau\\}}{\\Sigma_j\\exp\\{z_{cj}/\\tau\\}}).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When the temperature $\\tau$ is high compared with the magnitude of the logits, we can approximate $\\exp\\{z_{i}/\\tau\\}$ through taylor expansion, i.e. $\\exp\\{z_{i}/\\tau\\} \\approx 1+z_{i}/\\tau$. So the gradient can be estimated as following,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{H}(\\sigma(z_{ci}/\\tau), \\sigma(z_{di}/\\tau))}{\\partial z_{di}}\n",
    "&\\approx \\frac{1}{\\tau}(\\frac{1+z_{di}/\\tau}{N+\\Sigma_j z_{dj}/\\tau} - \\frac{1+z_{ci}/\\tau}{N+\\Sigma_j z_{cj}/\\tau}),\n",
    "\\end{align}\n",
    "$$\n",
    "where N is the number of total classes.\n",
    "\n",
    "When the logits are zero-meaned for each transfer case, i.e. $\\Sigma_j z_{dj} = \\Sigma_j z_{cj} = 0$, the gradient can be simplified to \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{H}(\\sigma(z_{ci}/\\tau), \\sigma(z_{di}/\\tau))}{\\partial z_{di}}\n",
    "&\\approx \\frac{1}{\\tau}(\\frac{1+z_{di}/\\tau}{N} - \\frac{1+z_{ci}/\\tau}{N})\\\\\n",
    "&\\approx \\frac{1}{N\\tau^2}(z_{di} - z_{ci}).\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So when the temperature is high, distillation is also minimizing $\\frac{1}{2}(z_{di} - z_{ci})$, the squared difference between the logits produced by the cumbersome model and those produced by the small model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Reimplementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training emsembles of specialists on CIFAR-100\n",
    "(Tutorial of Section 5 in the Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd.variable as Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import collections\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load CIFAR100 Dataset\n",
    "\n",
    "\n",
    "We used the CIFAR100 dataset to implement the algorithm introduced in Section 5 of the paper. The dataset that the paper used was a huge internal Google dataset(JFT). Since we do not have access to that, we used CIFAR100 that has similar properties(big and contains large number of labels) as the JFT dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size =4\n",
    "class_num = 100\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the resource of [CIFAR100](https://www.cs.toronto.edu/~kriz/cifar.html), there are 100 classes of images where each class contains 600 images. In particular, there are 500 training images and 100 testing images per class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalist Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,class_num=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    \n",
    "generalist_net = Net(class_num=class_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first built a generalist model on all the training data. The generalist model is simply a standard convolutional neural network with the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generalist_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we trained this generalist model using cross entropy loss and stochastic gradient descent for 5 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(generalist_net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527db798ede6407e85024efed6135921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.1690870308208465\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef2507af71f4d8197a4117abfadeadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.5994321661281585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d4bc7358d6d4096b99df99ca97ed2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.330149893140793\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17cb55e80e5047db90861addf11e99ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 4', max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.1707751224660874\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288f705766d748ec8040d503756c7b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 5', max=12500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 3.0574880642986297\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in tqdm(trainloader,desc='epoch '+str(epoch+1)):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = generalist_net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "#         if i % 4000 == 3999:    # print every 2000 mini-batches\n",
    "#             print('[%d, %5d] loss: %.3f' %\n",
    "#                   (epoch + 1, i + 1, running_loss / 2000))\n",
    "#             running_loss = 0.0\n",
    "    print('loss:',running_loss/len(trainloader))\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We checked the accuracy of the generalist model on test set and accuracy of each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 24 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = generalist_net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of       : 45 %\n",
      "Accuracy of       : 22 %\n",
      "Accuracy of       : 21 %\n",
      "Accuracy of       :  4 %\n",
      "Accuracy of       :  3 %\n",
      "Accuracy of       :  6 %\n",
      "Accuracy of       : 15 %\n",
      "Accuracy of       : 24 %\n",
      "Accuracy of       : 22 %\n",
      "Accuracy of       : 34 %\n",
      "Accuracy of       :  9 %\n",
      "Accuracy of       :  7 %\n",
      "Accuracy of       : 15 %\n",
      "Accuracy of       : 12 %\n",
      "Accuracy of       : 24 %\n",
      "Accuracy of       :  7 %\n",
      "Accuracy of       : 23 %\n",
      "Accuracy of       : 36 %\n",
      "Accuracy of       : 44 %\n",
      "Accuracy of       : 10 %\n",
      "Accuracy of       : 56 %\n",
      "Accuracy of       : 51 %\n",
      "Accuracy of       : 17 %\n",
      "Accuracy of       : 59 %\n",
      "Accuracy of       : 58 %\n",
      "Accuracy of       :  8 %\n",
      "Accuracy of       :  4 %\n",
      "Accuracy of       : 27 %\n",
      "Accuracy of       : 37 %\n",
      "Accuracy of       : 15 %\n",
      "Accuracy of       : 25 %\n",
      "Accuracy of       : 19 %\n",
      "Accuracy of       : 11 %\n",
      "Accuracy of       : 33 %\n",
      "Accuracy of       : 11 %\n",
      "Accuracy of       :  2 %\n",
      "Accuracy of       : 25 %\n",
      "Accuracy of       :  9 %\n",
      "Accuracy of       : 16 %\n",
      "Accuracy of       : 13 %\n",
      "Accuracy of       : 17 %\n",
      "Accuracy of       : 60 %\n",
      "Accuracy of       :  7 %\n",
      "Accuracy of       : 59 %\n",
      "Accuracy of       :  3 %\n",
      "Accuracy of       :  6 %\n",
      "Accuracy of       : 12 %\n",
      "Accuracy of       : 22 %\n",
      "Accuracy of       : 40 %\n",
      "Accuracy of       : 27 %\n",
      "Accuracy of       :  3 %\n",
      "Accuracy of       : 23 %\n",
      "Accuracy of       : 71 %\n",
      "Accuracy of       : 40 %\n",
      "Accuracy of       : 35 %\n",
      "Accuracy of       :  2 %\n",
      "Accuracy of       : 29 %\n",
      "Accuracy of       : 21 %\n",
      "Accuracy of       : 13 %\n",
      "Accuracy of       : 27 %\n",
      "Accuracy of       : 67 %\n",
      "Accuracy of       : 47 %\n",
      "Accuracy of       : 27 %\n",
      "Accuracy of       : 25 %\n",
      "Accuracy of       :  3 %\n",
      "Accuracy of       : 14 %\n",
      "Accuracy of       : 27 %\n",
      "Accuracy of       : 17 %\n",
      "Accuracy of       : 59 %\n",
      "Accuracy of       : 46 %\n",
      "Accuracy of       : 49 %\n",
      "Accuracy of       : 57 %\n",
      "Accuracy of       :  5 %\n",
      "Accuracy of       : 23 %\n",
      "Accuracy of       :  2 %\n",
      "Accuracy of       : 39 %\n",
      "Accuracy of       : 60 %\n",
      "Accuracy of       : 12 %\n",
      "Accuracy of       :  6 %\n",
      "Accuracy of       : 23 %\n",
      "Accuracy of       :  2 %\n",
      "Accuracy of       : 26 %\n",
      "Accuracy of       : 47 %\n",
      "Accuracy of       : 23 %\n",
      "Accuracy of       : 11 %\n",
      "Accuracy of       : 53 %\n",
      "Accuracy of       : 33 %\n",
      "Accuracy of       : 33 %\n",
      "Accuracy of       :  9 %\n",
      "Accuracy of       : 39 %\n",
      "Accuracy of       : 24 %\n",
      "Accuracy of       : 28 %\n",
      "Accuracy of       :  0 %\n",
      "Accuracy of       : 13 %\n",
      "Accuracy of       : 50 %\n",
      "Accuracy of       : 42 %\n",
      "Accuracy of       :  6 %\n",
      "Accuracy of       : 38 %\n",
      "Accuracy of       :  5 %\n",
      "Accuracy of       :  5 %\n"
     ]
    }
   ],
   "source": [
    "class_correct = list(0. for i in range(class_num))\n",
    "class_total = list(0. for i in range(class_num))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = generalist_net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(class_num):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign classes to specialists using Kmeans\n",
    "\n",
    "According to the paper, each specialist takes a group of classes that the generalist model often confuses. These groups are obtained by applying K-means algorithm to the columns of the covariance matrix of generalists model predictions. We implemented this clustering process below and we chose 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 3 3 2 2 1 3 3 1 3 3 3 0 1 3 2 3 0 2 2 3 2 3 0 3 1 3 2 3 2 0 2 1 1 3 3 3\n",
      " 1 2 1 3 3 2 3 2 3 3 1 1 0 2 3 0 3 3 2 0 3 1 0 0 3 3 2 2 2 2 0 0 0 3 0 2 0\n",
      " 2 2 0 2 3 2 2 1 3 3 3 0 3 1 3 1 1 1 3 2 3 0 0 2 3 3]\n"
     ]
    }
   ],
   "source": [
    "cluster_num = 4\n",
    "\n",
    "all_output = []\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        outputs = generalist_net(images)\n",
    "        all_output.append(outputs)\n",
    "all_output = torch.cat(all_output)\n",
    "\n",
    "# print(all_output.shape)\n",
    "\n",
    "img = all_output.detach().numpy().transpose()\n",
    "# print(img.shape)\n",
    "cov_pred = np.cov(img)\n",
    "kmeans = KMeans(n_clusters=cluster_num).fit(cov_pred)\n",
    "print(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_cluster = collections.defaultdict()\n",
    "cluster_to_label = collections.defaultdict(list)\n",
    "for original_label, cluster in enumerate(kmeans.labels_):\n",
    "    label_to_cluster[original_label] = cluster\n",
    "    cluster_to_label[cluster].append(original_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also split the dataset for specialist models as mentioned the paper. Each specialist was trined on a subset of the full dataset with half its examples coming from its specialist target classes and half sampled at random from the rest of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialists_trainsets = collections.defaultdict(list)\n",
    "specialists_dustbin_from = collections.defaultdict(list)\n",
    "                     \n",
    "for i,data in enumerate(trainset,0):\n",
    "    image, label = data\n",
    "    cluster = label_to_cluster[label]\n",
    "    specialists_trainsets[cluster].append((image, cluster_to_label[cluster].index(label)))\n",
    "    for c in range(cluster_num):\n",
    "        if c != cluster:\n",
    "            specialists_dustbin_from[c].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the specialist model is similar to the generalist model since it will be initialized with the weights from the generalist model. Note that each specialist model will have its unique dimension of output depending on the number of classes in its special group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialistNet(nn.Module):\n",
    "    def __init__(self,specialist_class_num):\n",
    "        super(SpecialistNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, specialist_class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Train Specialists\n",
    "\n",
    "We initialized each specialist with the weights of the generalist model. Then we trained them with half data from the special subset and half sampled from the remaining of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialist_models = collections.defaultdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f8bb7134c648f2ad7be1cbbe72ba70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=4500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.2319106579158041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ba59a380734f53b4d843435a50a778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=4500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0511562505430645\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302a45a4373e43ecb6950a4c949a20aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=4500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9776741216712528\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6742e68f3449df8090e4b3a4fea0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 4', max=4500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.9207407252258725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2542f987c4854c3a971cb0f023dcf419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 5', max=4500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.8763322533435292\n",
      "\n",
      "Finished Training Cluster # 0\n",
      "Accuracy of specialist # 0 on training set: 72.83888888888889%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a569a371828487ba782c27d08cda8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.565105987548828\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f042b7b4f5b48ed89e2063d88d402ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.3402041165456176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141130b4e8b7495c87692d6e7a6ca25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.2417551368400455\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "106c2b9e1c8c43fdbbdf4cb9c94745c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 4', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.1679255098924042\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3dce2a5c1aa43e7bef74e11eb07c7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 5', max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0984917037189006\n",
      "\n",
      "Finished Training Cluster # 1\n",
      "Accuracy of specialist # 1 on training set: 70.0875%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19060e3e167457ca5ff5ef2d2fcfb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=6500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.9742311274546844\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89e5ed7562fd433bac779d87c25157d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=6500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.7785099202577885\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2a6d94483e48469eedea12e6c33882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=6500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.687916194659013\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78d5a4756e94e9a889daacfad3db148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 4', max=6500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.6180490029775179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcedb9981984c93b93596ce7d604704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 5', max=6500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5601374145058484\n",
      "\n",
      "Finished Training Cluster # 2\n",
      "Accuracy of specialist # 2 on training set: 58.738461538461536%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac0a01c81af4e99ab79f0a871f2638c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 1', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.9777923901498318\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029931b2613246bbb1a526da09034e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 2', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.7240972446799279\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a00570e9426485b9e713ef6db00d553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 3', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.6254383429646493\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f530d05d276b4d91ab68f0a151475f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 4', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.554765300154686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72906824844946ea896efafeeca5a1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='epoch 5', max=10000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.5032051890432834\n",
      "\n",
      "Finished Training Cluster # 3\n",
      "Accuracy of specialist # 3 on training set: 61.3575%\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(cluster_num):\n",
    "    specialist_dustbin_indices = np.random.choice(len(specialists_dustbin_from[cluster]),\n",
    "                                                  len(specialists_trainsets[cluster]),\n",
    "                                                  replace=False)\n",
    "    dustbin_indices = [specialists_dustbin_from[cluster][i] for i in specialist_dustbin_indices]\n",
    "    sampler = SubsetRandomSampler(dustbin_indices)\n",
    "    specialist_dustbin = torch.utils.data.DataLoader(trainset, sampler = sampler,num_workers=2)\n",
    "    \n",
    "    # relabel specialist trainset and dustbin\n",
    "    specialist_dustbin = [(data[0][0],len(cluster_to_label[cluster])) for data in specialist_dustbin]\n",
    "    \n",
    "    specialist_trainset_full = specialists_trainsets[cluster] + specialist_dustbin\n",
    "#     print(len(specialist_trainset_full))\n",
    "    \n",
    "    specialist_trainloader = torch.utils.data.DataLoader(specialist_trainset_full, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "    \n",
    "    \n",
    "    specialist_net = SpecialistNet(specialist_class_num=len(cluster_to_label[cluster])+1)\n",
    "    \n",
    "    specialist_net.conv1.weight.data = generalist_net.conv1.weight\n",
    "    specialist_net.conv2.weight.data = generalist_net.conv2.weight\n",
    "    specialist_net.fc1.weight.data = generalist_net.fc1.weight\n",
    "    specialist_net.fc2.weight.data = generalist_net.fc2.weight\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(specialist_net.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    \n",
    "    for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for data in tqdm(specialist_trainloader,desc='epoch '+str(epoch+1)):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "\n",
    "    #         print(inputs, labels)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = specialist_net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "#             if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "#                 print('[%d, %5d] loss: %.3f' %\n",
    "#                       (epoch + 1, i + 1, running_loss / 2000))\n",
    "#                 running_loss = 0.0\n",
    "        print('loss:',running_loss/len(specialist_trainloader))\n",
    "    print('\\nFinished Training Cluster #',str(cluster))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in specialist_trainloader:\n",
    "            images, labels = data\n",
    "            outputs = specialist_net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of specialist # '+str(cluster)+' on training set: '\n",
    "          +str(100 * correct / total)+'%')\n",
    "    \n",
    "    specialist_models[cluster] = specialist_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing inference with ensembles of specialists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine the generalist and the specialists, we performed two steps:\n",
    "\n",
    "* **step 1**: For each test case, we find the n most probable classes according to the generalist model.\n",
    "Call this set of classes k. In our experiments, we used n = 3.\n",
    "\n",
    "* **step 2**: Now, we have all the specialists, m, which have a special subset of confusable classes, $S^m$. The subset $S^m$ has a non-empty intersection with k and we call this $A_k$. For each test case, we want to find a full probability distribution $q$ over all the 100 classes which minimize the distances between $p_g$ the probability distribution of the generalist and $p_m$ the probability distribution of each specialist. More specifically, we want to minimize the loss function: $$KL(p^g,q)+\\sum_{m \\in A_k}{KL(p^m,q)}$$. According to the paper, the solution is either the arithmetic or geometric mean of $p^g$ and $p^m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialist_result_expansion(specialist,specialist_result,class_num):\n",
    "    expanded = torch.zeros(class_num)\n",
    "    specialist_member_num = len(specialist)-1\n",
    "    for i in range(class_num):\n",
    "        if i in specialist:\n",
    "            expanded[i] = specialist_result[specialist.index(i)]\n",
    "        else:\n",
    "            expanded[i] = specialist_result[-1] / (class_num-specialist_member_num)\n",
    "    return expanded\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute p_g, p_m, and perform GD to find q\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "total_gen = 0\n",
    "correct_gen = 0\n",
    "\n",
    "n = 3 # consider top n probable classes from generalist model\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = generalist_net(images)\n",
    "        \n",
    "        # z distribution, label for a batch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        topn_z,topn_predicted = torch.topk(outputs,n)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # probability distribution for a batch\n",
    "        p_g = F.softmax(outputs, dim = 0)\n",
    "        for i in range(batch_size):\n",
    "            topn_sum = []\n",
    "            for j in range(n):\n",
    "                generalist_label = topn_predicted[i][j]\n",
    "#                 generalist_label = predicted[i]\n",
    "                cluster = label_to_cluster[int(generalist_label)]\n",
    "                specialist_net = specialist_models[int(cluster)]\n",
    "                specialist_outputs = specialist_net(images)\n",
    "#                 pg_i = outputs[i] # z\n",
    "#                 pm_i = specialist_outputs[i] #z\n",
    "                \n",
    "                pg_i = p_g[i] #softmax\n",
    "                pm_i = F.softmax(specialist_outputs[i],dim=0) #softmax\n",
    "                \n",
    "                expanded_pm_i = specialist_result_expansion(cluster_to_label[cluster],pm_i,class_num=class_num)\n",
    "                topn_sum.append(expanded_pm_i.numpy())\n",
    "#             q_hat=np.mean([pg_i.numpy(),expanded_pm_i.numpy()],axis=0)\n",
    "            q_hat = np.mean([pg_i.numpy()]+topn_sum,axis=0)\n",
    "#             q_hat = F.softmax(torch.Tensor(q_hat),dim=0)\n",
    "#             q_hat = expanded_pm_i\n",
    "            label = np.argmax(q_hat)\n",
    "\n",
    "            total += 1\n",
    "            if label == labels[i]:\n",
    "                correct += 1\n",
    "\n",
    "\n",
    "        total_gen += labels.size(0)\n",
    "        correct_gen += (predicted == labels).sum().item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy of generalist:  12.7 %\n",
      "test accuracy of ensemble:  15.64 %\n"
     ]
    }
   ],
   "source": [
    "print(\"test accuracy of generalist: \", 100*correct_gen/total_gen,'%')\n",
    "print(\"test accuracy of ensemble: \", 100*correct/total,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, the paper found the full probability distribution $q$ over all the classes that minimizes $KL(p^g,q)+\\sum_{m \\in A_k}{KL(p^m,q)}$. It also mentioned that they tried to use gradient descent to find such $q$. So we tried to implement this part as well. However, due to the difference underline property(much less total data and number of classes) of our CIFAR100 dataset and their JFT dataset, using gradient descent to optimize $q$ didn't work that well. As a result, we didn't show this part in the tutorial above. Please find some of the utility functions we wrote for that part below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_q(P_g,q,P_m):\n",
    "    # q should have size same as P_g(10)\n",
    "    # need to sum all dustbin class probabilities into one\n",
    "    \n",
    "    if len(P_m) == len(animals)+1:\n",
    "        q_resize = torch.zeros(len(animals)+1,dtype=torch.float)\n",
    "        for i,index in enumerate(animals):\n",
    "            q_resize[i] = q[index]\n",
    "        for j,index in enumerate(transportation):\n",
    "            q_resize[-1] += q[index]\n",
    "    elif len(P_m) == len(transportation)+1:\n",
    "        q_resize = torch.zeros(len(transportation)+1,dtype=torch.float)\n",
    "        for i,index in enumerate(transportation):\n",
    "            q_resize[i] = q[index]\n",
    "        for j,index in enumerate(animals):\n",
    "            q_resize[-1] += q[index]\n",
    "    return q_resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_loss(P_g,q,P_m,q_resize):\n",
    "    return F.kl_div(P_g,q) + F.kl_div(P_m,q_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_gradient(P_g,P_m,q,q_resize,special_list,num_class = 10):\n",
    "    # make sure P_q, P_m, and q are all Tensors\n",
    "    gradient_Pg = - P_g / q\n",
    "    q_dustbin = 0\n",
    "    # calculate q_dustbin\n",
    "    for i in range(num_class):\n",
    "        if i not in special_list:\n",
    "            q_dustbin += q[i]\n",
    "    gradient_Pm = torch.zeros(num_class)\n",
    "    for i in range(num_class):\n",
    "        if i in special_list:\n",
    "            gradient_Pm[i] = -P_m[special_list.index(i)]/q[i]\n",
    "        else:\n",
    "            gradient_Pm[i] = -P_m[-1]/q_dustbin\n",
    "    return gradient_Pg + gradient_Pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimization(P_g,P_m,num_epoch):\n",
    "    \n",
    "    if len(P_m) == len(animals)+1:\n",
    "        special_list = animals\n",
    "    elif len(P_m) == len(transportation)+1:\n",
    "        special_list = transportation\n",
    "    \n",
    "    gamma = 0.01 # step size\n",
    "    \n",
    "    \n",
    "    \n",
    "    q = Variable(np.random.rand(10),dtype = torch.float, requires_grad=True) # random initialize q\n",
    "#     print('random q:',q)\n",
    "    q_resize = resize_q(P_g,q,P_m)\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        loss = KL_loss(P_g,q,P_m,q_resize)\n",
    "#         print('LOSS:',loss)\n",
    "        q -= gamma * KL_gradient(P_g,P_m,q,q_resize,special_list)\n",
    "        \n",
    "#         print('updated? q:',q)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the training time of both generalist and specialist models, the specialists train much faster than the generalist, because each specialist only targets one subset of confusable classes. All of them are trained completely independently. The baseline generalist model achieved x test accuracy, while the ensemble system of generalist model combined with the specialists achieved x test accuracy, which shows a % relative improvment in test accuracy overall.\n",
    "\n",
    "For the CIFAR100 experiments, we first used kmeans algorithm to split the 100 classes into 4 clusters and then trained a specialist for each cluster. We then tried 10 clusters and built 10 specialists. By comparing them, we found that the test accuracy improvements are larger when we have more specialists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
