{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Distillation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "68pgpfK8Rmxg",
        "colab_type": "code",
        "outputId": "f39cbb72-1c87-4e06-d866-9f0970e05ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision\n",
        "\n",
        "import torch\n",
        "print (torch.cuda.get_device_name(0))\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import sys\n",
        "from torch.utils.data.dataset import Dataset\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.4.1 from http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
            "\u001b[?25l  Downloading http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl (512.6MB)\n",
            "\u001b[K    100% |████████████████████████████████| 512.6MB 50.2MB/s \n",
            "tcmalloc: large alloc 1073750016 bytes == 0x58bfe000 @  0x7f30db6382a4 0x591a07 0x5b5d56 0x502e9a 0x506859 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x502209 0x502f3d 0x506859 0x504c28 0x502540 0x502f3d 0x507641 0x504c28 0x502540 0x502f3d 0x507641\n",
            "\u001b[?25hInstalling collected packages: torch\n",
            "Successfully installed torch-0.4.1\n",
            "Collecting torchvision\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 4.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Collecting pillow>=4.1.1 (from torchvision)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/94/5430ebaa83f91cc7a9f687ff5238e26164a779cca2ef9903232268b0a318/Pillow-5.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.0MB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Installing collected packages: pillow, torchvision\n",
            "  Found existing installation: Pillow 4.0.0\n",
            "    Uninstalling Pillow-4.0.0:\n",
            "      Successfully uninstalled Pillow-4.0.0\n",
            "Successfully installed pillow-5.3.0 torchvision-0.2.1\n",
            "Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rXY8Y-LJoSQ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "19fd0ae4-6d8a-49eb-8cc0-89fdb27b36a3"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# MNIST dataset \n",
        "train_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),  \n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='../../data', \n",
        "                                          train=False, \n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LqPQfyn8oSV2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LogitDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        # stuff\n",
        "        self.data = []\n",
        "        self.soft_targets = []\n",
        "        self.hard_targets = []\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # stuff\n",
        "        return (self.data[index], self.soft_targets[index], self.hard_targets[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "     \n",
        "    def add(self, img, soft_label, hard_label):\n",
        "        self.data.append(img)\n",
        "        self.soft_targets.append(soft_label)\n",
        "        self.hard_targets.append(hard_label)\n",
        "        \n",
        "        \n",
        "    def stackAll(self):\n",
        "        self.data = torch.stack(self.data).reshape(-1, 1, 28, 28)\n",
        "        self.soft_targets = torch.stack(self.soft_targets).reshape(-1, 10)\n",
        "        self.hard_targets = torch.stack(self.hard_targets).reshape(-1)\n",
        "\n",
        "      \n",
        "def cross_entropy(logsoftmax_pred, soft_targets, hard_pred, hard_targets, alpha):\n",
        "    cross_entropy_soft = torch.mean(torch.sum(- soft_targets * logsoftmax_pred, 1))\n",
        "    \n",
        "    hard_cross_entropy_criterion = nn.CrossEntropyLoss()\n",
        "    cross_entropy_hard = hard_cross_entropy_criterion(hard_pred, hard_targets)\n",
        "    \n",
        "    return torch.mul(cross_entropy_soft, alpha) + torch.mul(cross_entropy_hard, 1.0-alpha)\n",
        "  \n",
        "\n",
        "\n",
        "# Convolutional neural network (two convolutional layers)\n",
        "class TeacherNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(TeacherNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.fc = nn.Linear(7*7*32, num_classes)\n",
        "        self.dropout_1 = nn.Dropout(p=0.25)\n",
        "        self.dropout_2 = nn.Dropout(p=0.25)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.dropout_1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.dropout_2(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "class StudentNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(StudentNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size) \n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes)  \n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0eu3ZQcuoSZ3",
        "colab_type": "code",
        "outputId": "f13862bc-9858-4730-fa6f-a579c27b1076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3539
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyper-parameters \n",
        "input_size = 784\n",
        "hidden_size = 1200\n",
        "num_classes = 10\n",
        "num_epochs = 100\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "train_temp = 4.0 \n",
        "teacher_hidden = 1200\n",
        "student_hidden = 20\n",
        "weight_ratio = 0.9\n",
        "CNN_TRAIN_EPOCH_LIMIT = 100\n",
        "\n",
        "# Initialize teacher and student model\n",
        "teacher_model = TeacherNet(num_classes).to(device)\n",
        "student_vanila_model = StudentNet(input_size, student_hidden, num_classes).to(device)\n",
        "student_enhanced_model = StudentNet(input_size, student_hidden, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer for teacher\n",
        "criterion_teacher = nn.CrossEntropyLoss()\n",
        "optimizer_teacher = torch.optim.Adam(teacher_model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Loss and optimizer for student vanila\n",
        "criterion_student_vanila = nn.CrossEntropyLoss()\n",
        "optimizer_student_vanila = torch.optim.Adam(student_vanila_model.parameters(), lr=learning_rate)  \n",
        "\n",
        "# Train the CNN Teacher Model\n",
        "\n",
        "# Train the vanila Student Model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images_for_teacher = images.to(device)\n",
        "        images_for_student = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Teacher Network\n",
        "        if epoch < CNN_TRAIN_EPOCH_LIMIT:\n",
        "          outputs_teacher = teacher_model(images_for_teacher) # teacher softmax\n",
        "          loss_teacher = criterion_teacher(outputs_teacher, labels)\n",
        "\n",
        "          optimizer_teacher.zero_grad()\n",
        "          loss_teacher.backward()\n",
        "          optimizer_teacher.step()\n",
        "        \n",
        "        \n",
        "        # Student Network\n",
        "        outputs_student_vanila = student_vanila_model(images_for_student) # student softmax\n",
        "        loss_student_vanila = criterion_student_vanila (outputs_student_vanila, labels)\n",
        "        \n",
        "        optimizer_student_vanila.zero_grad()\n",
        "        loss_student_vanila.backward()\n",
        "        optimizer_student_vanila.step()\n",
        "        \n",
        "        if (i+1) % 600 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss Teacher: {:.4f}, Loss Student: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss_teacher.item(), loss_student_vanila.item()))\n",
        "\n",
        "# Test the model\n",
        "teacher_model = teacher_model.eval()\n",
        "student_vanila_model = student_vanila_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct1, correct2 = 0, 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        \n",
        "        images_for_teacher = images.to(device)\n",
        "        images_for_student = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        output1, output2 = teacher_model(images_for_teacher), student_vanila_model(images_for_student)\n",
        "        _, predicted1 = torch.max(output1.data, 1)\n",
        "        _, predicted2 = torch.max(output2.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct1 += (predicted1 == labels).sum().item()\n",
        "        correct2 += (predicted2 == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: TEACHER: {}, STUDENT: {} %'.format(100 * correct1 / total, 100 * correct2 / total))\n",
        "    \n",
        "print (\"CREATING NEW DATASET WITH LOGIT LABEL...\")\n",
        "logit_dataset = LogitDataset()\n",
        "with torch.no_grad():\n",
        "    for images, labels in train_loader:\n",
        "        logit_image = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        output1 = teacher_model(logit_image)\n",
        "        \n",
        "        logit_soft_label = output1.data\n",
        "        logit_hard_label = labels.data\n",
        "        \n",
        "        logit_dataset.add(logit_image, logit_soft_label, logit_hard_label)\n",
        "    \n",
        "    logit_dataset.stackAll()\n",
        "\n",
        "print (\"TRAINING ENHANCED STUDENT WITH SOFTMAX LABEL...\")\n",
        "train_loader_logit = torch.utils.data.DataLoader(dataset=logit_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "# Loss and optimizer for student enhanced\n",
        "criterion_student_enhanced = cross_entropy\n",
        "optimizer_student_enhanced = torch.optim.Adam(student_enhanced_model.parameters(), lr=learning_rate) \n",
        "\n",
        "total_step = len(train_loader_logit)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, soft_labels, hard_labels) in enumerate(train_loader_logit):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        soft_labels = soft_labels.to(device)\n",
        "        hard_labels = hard_labels.to(device)\n",
        "        \n",
        "        outputs_student_enhanced = student_enhanced_model(images)\n",
        "\n",
        "        # Apply Softmax with Temperature\n",
        "        softmax = nn.Softmax()\n",
        "        logSoftmax = nn.LogSoftmax()\n",
        "        \n",
        "        softmax_outputs_teacher = softmax(torch.div(soft_labels, train_temp))\n",
        "        log_softmax_output_student = logSoftmax(torch.div(outputs_student_enhanced, train_temp))\n",
        "\n",
        "        loss_student_enhanced = criterion_student_enhanced (log_softmax_output_student, softmax_outputs_teacher, outputs_student_enhanced, hard_labels, weight_ratio)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer_student_enhanced.zero_grad()\n",
        "        loss_student_enhanced.backward()\n",
        "        optimizer_student_enhanced.step()\n",
        "        \n",
        "        if (i+1) % 600 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss Enhanced Student: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss_student_enhanced.item()))\n",
        "\n",
        "# Test the model\n",
        "student_enhanced_model = student_enhanced_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct1 = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, 28*28).to(device)\n",
        "        labels = labels.to(device)\n",
        "        output1 = student_enhanced_model(images)\n",
        "        _, predicted1 = torch.max(output1.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct1 += (predicted1 == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: STUDENT ENHANCED: {} %'.format(100 * correct1 / total))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Step [600/600], Loss Teacher: 0.0606, Loss Student: 0.2553\n",
            "Epoch [2/100], Step [600/600], Loss Teacher: 0.0158, Loss Student: 0.1775\n",
            "Epoch [3/100], Step [600/600], Loss Teacher: 0.0431, Loss Student: 0.1917\n",
            "Epoch [4/100], Step [600/600], Loss Teacher: 0.0286, Loss Student: 0.2152\n",
            "Epoch [5/100], Step [600/600], Loss Teacher: 0.0891, Loss Student: 0.2553\n",
            "Epoch [6/100], Step [600/600], Loss Teacher: 0.0341, Loss Student: 0.1825\n",
            "Epoch [7/100], Step [600/600], Loss Teacher: 0.0238, Loss Student: 0.0862\n",
            "Epoch [8/100], Step [600/600], Loss Teacher: 0.0483, Loss Student: 0.1951\n",
            "Epoch [9/100], Step [600/600], Loss Teacher: 0.0047, Loss Student: 0.1757\n",
            "Epoch [10/100], Step [600/600], Loss Teacher: 0.0046, Loss Student: 0.0908\n",
            "Epoch [11/100], Step [600/600], Loss Teacher: 0.0286, Loss Student: 0.1363\n",
            "Epoch [12/100], Step [600/600], Loss Teacher: 0.0707, Loss Student: 0.1580\n",
            "Epoch [13/100], Step [600/600], Loss Teacher: 0.0092, Loss Student: 0.1551\n",
            "Epoch [14/100], Step [600/600], Loss Teacher: 0.0225, Loss Student: 0.1274\n",
            "Epoch [15/100], Step [600/600], Loss Teacher: 0.0382, Loss Student: 0.2345\n",
            "Epoch [16/100], Step [600/600], Loss Teacher: 0.0367, Loss Student: 0.0427\n",
            "Epoch [17/100], Step [600/600], Loss Teacher: 0.0171, Loss Student: 0.0903\n",
            "Epoch [18/100], Step [600/600], Loss Teacher: 0.0121, Loss Student: 0.1923\n",
            "Epoch [19/100], Step [600/600], Loss Teacher: 0.0541, Loss Student: 0.0690\n",
            "Epoch [20/100], Step [600/600], Loss Teacher: 0.0524, Loss Student: 0.1300\n",
            "Epoch [21/100], Step [600/600], Loss Teacher: 0.0021, Loss Student: 0.0173\n",
            "Epoch [22/100], Step [600/600], Loss Teacher: 0.0130, Loss Student: 0.1517\n",
            "Epoch [23/100], Step [600/600], Loss Teacher: 0.0157, Loss Student: 0.0832\n",
            "Epoch [24/100], Step [600/600], Loss Teacher: 0.0400, Loss Student: 0.2051\n",
            "Epoch [25/100], Step [600/600], Loss Teacher: 0.0138, Loss Student: 0.1289\n",
            "Epoch [26/100], Step [600/600], Loss Teacher: 0.0420, Loss Student: 0.1336\n",
            "Epoch [27/100], Step [600/600], Loss Teacher: 0.0332, Loss Student: 0.0939\n",
            "Epoch [28/100], Step [600/600], Loss Teacher: 0.0136, Loss Student: 0.1444\n",
            "Epoch [29/100], Step [600/600], Loss Teacher: 0.0185, Loss Student: 0.1988\n",
            "Epoch [30/100], Step [600/600], Loss Teacher: 0.0328, Loss Student: 0.0486\n",
            "Epoch [31/100], Step [600/600], Loss Teacher: 0.0291, Loss Student: 0.1044\n",
            "Epoch [32/100], Step [600/600], Loss Teacher: 0.0027, Loss Student: 0.0281\n",
            "Epoch [33/100], Step [600/600], Loss Teacher: 0.2164, Loss Student: 0.0996\n",
            "Epoch [34/100], Step [600/600], Loss Teacher: 0.0024, Loss Student: 0.1593\n",
            "Epoch [35/100], Step [600/600], Loss Teacher: 0.0523, Loss Student: 0.0493\n",
            "Epoch [36/100], Step [600/600], Loss Teacher: 0.0036, Loss Student: 0.1232\n",
            "Epoch [37/100], Step [600/600], Loss Teacher: 0.0203, Loss Student: 0.0957\n",
            "Epoch [38/100], Step [600/600], Loss Teacher: 0.0006, Loss Student: 0.0095\n",
            "Epoch [39/100], Step [600/600], Loss Teacher: 0.0132, Loss Student: 0.0442\n",
            "Epoch [40/100], Step [600/600], Loss Teacher: 0.0134, Loss Student: 0.0332\n",
            "Epoch [41/100], Step [600/600], Loss Teacher: 0.0583, Loss Student: 0.1120\n",
            "Epoch [42/100], Step [600/600], Loss Teacher: 0.0033, Loss Student: 0.0289\n",
            "Epoch [43/100], Step [600/600], Loss Teacher: 0.0028, Loss Student: 0.0459\n",
            "Epoch [44/100], Step [600/600], Loss Teacher: 0.0459, Loss Student: 0.0849\n",
            "Epoch [45/100], Step [600/600], Loss Teacher: 0.0347, Loss Student: 0.0971\n",
            "Epoch [46/100], Step [600/600], Loss Teacher: 0.0034, Loss Student: 0.0085\n",
            "Epoch [47/100], Step [600/600], Loss Teacher: 0.0004, Loss Student: 0.0261\n",
            "Epoch [48/100], Step [600/600], Loss Teacher: 0.0163, Loss Student: 0.0193\n",
            "Epoch [49/100], Step [600/600], Loss Teacher: 0.0391, Loss Student: 0.2399\n",
            "Epoch [50/100], Step [600/600], Loss Teacher: 0.0124, Loss Student: 0.0273\n",
            "Epoch [51/100], Step [600/600], Loss Teacher: 0.0319, Loss Student: 0.0729\n",
            "Epoch [52/100], Step [600/600], Loss Teacher: 0.0307, Loss Student: 0.0782\n",
            "Epoch [53/100], Step [600/600], Loss Teacher: 0.0054, Loss Student: 0.0582\n",
            "Epoch [54/100], Step [600/600], Loss Teacher: 0.0066, Loss Student: 0.0215\n",
            "Epoch [55/100], Step [600/600], Loss Teacher: 0.0049, Loss Student: 0.0159\n",
            "Epoch [56/100], Step [600/600], Loss Teacher: 0.0371, Loss Student: 0.0306\n",
            "Epoch [57/100], Step [600/600], Loss Teacher: 0.0122, Loss Student: 0.0632\n",
            "Epoch [58/100], Step [600/600], Loss Teacher: 0.0056, Loss Student: 0.0345\n",
            "Epoch [59/100], Step [600/600], Loss Teacher: 0.0376, Loss Student: 0.0231\n",
            "Epoch [60/100], Step [600/600], Loss Teacher: 0.0020, Loss Student: 0.0235\n",
            "Epoch [61/100], Step [600/600], Loss Teacher: 0.1884, Loss Student: 0.0583\n",
            "Epoch [62/100], Step [600/600], Loss Teacher: 0.0097, Loss Student: 0.0908\n",
            "Epoch [63/100], Step [600/600], Loss Teacher: 0.0020, Loss Student: 0.0541\n",
            "Epoch [64/100], Step [600/600], Loss Teacher: 0.0183, Loss Student: 0.0402\n",
            "Epoch [65/100], Step [600/600], Loss Teacher: 0.0008, Loss Student: 0.0059\n",
            "Epoch [66/100], Step [600/600], Loss Teacher: 0.0005, Loss Student: 0.0459\n",
            "Epoch [67/100], Step [600/600], Loss Teacher: 0.0008, Loss Student: 0.0117\n",
            "Epoch [68/100], Step [600/600], Loss Teacher: 0.0110, Loss Student: 0.0202\n",
            "Epoch [69/100], Step [600/600], Loss Teacher: 0.0032, Loss Student: 0.0066\n",
            "Epoch [70/100], Step [600/600], Loss Teacher: 0.0357, Loss Student: 0.1272\n",
            "Epoch [71/100], Step [600/600], Loss Teacher: 0.0169, Loss Student: 0.0184\n",
            "Epoch [72/100], Step [600/600], Loss Teacher: 0.0388, Loss Student: 0.0046\n",
            "Epoch [73/100], Step [600/600], Loss Teacher: 0.0055, Loss Student: 0.0672\n",
            "Epoch [74/100], Step [600/600], Loss Teacher: 0.0267, Loss Student: 0.0394\n",
            "Epoch [75/100], Step [600/600], Loss Teacher: 0.0663, Loss Student: 0.1713\n",
            "Epoch [76/100], Step [600/600], Loss Teacher: 0.0066, Loss Student: 0.0253\n",
            "Epoch [77/100], Step [600/600], Loss Teacher: 0.0023, Loss Student: 0.0358\n",
            "Epoch [78/100], Step [600/600], Loss Teacher: 0.0107, Loss Student: 0.0168\n",
            "Epoch [79/100], Step [600/600], Loss Teacher: 0.0016, Loss Student: 0.0305\n",
            "Epoch [80/100], Step [600/600], Loss Teacher: 0.0058, Loss Student: 0.0258\n",
            "Epoch [81/100], Step [600/600], Loss Teacher: 0.0282, Loss Student: 0.0161\n",
            "Epoch [82/100], Step [600/600], Loss Teacher: 0.0007, Loss Student: 0.0271\n",
            "Epoch [83/100], Step [600/600], Loss Teacher: 0.0043, Loss Student: 0.0828\n",
            "Epoch [84/100], Step [600/600], Loss Teacher: 0.0040, Loss Student: 0.0269\n",
            "Epoch [85/100], Step [600/600], Loss Teacher: 0.0022, Loss Student: 0.0494\n",
            "Epoch [86/100], Step [600/600], Loss Teacher: 0.0015, Loss Student: 0.1050\n",
            "Epoch [87/100], Step [600/600], Loss Teacher: 0.0003, Loss Student: 0.0135\n",
            "Epoch [88/100], Step [600/600], Loss Teacher: 0.0022, Loss Student: 0.0291\n",
            "Epoch [89/100], Step [600/600], Loss Teacher: 0.0006, Loss Student: 0.0157\n",
            "Epoch [90/100], Step [600/600], Loss Teacher: 0.0031, Loss Student: 0.0188\n",
            "Epoch [91/100], Step [600/600], Loss Teacher: 0.0047, Loss Student: 0.0274\n",
            "Epoch [92/100], Step [600/600], Loss Teacher: 0.0043, Loss Student: 0.0052\n",
            "Epoch [93/100], Step [600/600], Loss Teacher: 0.0041, Loss Student: 0.0079\n",
            "Epoch [94/100], Step [600/600], Loss Teacher: 0.0004, Loss Student: 0.0157\n",
            "Epoch [95/100], Step [600/600], Loss Teacher: 0.0128, Loss Student: 0.0285\n",
            "Epoch [96/100], Step [600/600], Loss Teacher: 0.0100, Loss Student: 0.0293\n",
            "Epoch [97/100], Step [600/600], Loss Teacher: 0.0029, Loss Student: 0.0152\n",
            "Epoch [98/100], Step [600/600], Loss Teacher: 0.0131, Loss Student: 0.0231\n",
            "Epoch [99/100], Step [600/600], Loss Teacher: 0.0003, Loss Student: 0.0074\n",
            "Epoch [100/100], Step [600/600], Loss Teacher: 0.0000, Loss Student: 0.0056\n",
            "Accuracy of the network on the 10000 test images: TEACHER: 99.37, STUDENT: 95.86 %\n",
            "CREATING NEW DATASET WITH LOGIT LABEL...\n",
            "TRAINING ENHANCED STUDENT WITH SOFTMAX LABEL...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:118: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:119: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Step [600/600], Loss Enhanced Student: 0.6191\n",
            "Epoch [2/100], Step [600/600], Loss Enhanced Student: 0.4313\n",
            "Epoch [3/100], Step [600/600], Loss Enhanced Student: 0.6131\n",
            "Epoch [4/100], Step [600/600], Loss Enhanced Student: 0.4827\n",
            "Epoch [5/100], Step [600/600], Loss Enhanced Student: 0.4700\n",
            "Epoch [6/100], Step [600/600], Loss Enhanced Student: 0.5517\n",
            "Epoch [7/100], Step [600/600], Loss Enhanced Student: 0.4702\n",
            "Epoch [8/100], Step [600/600], Loss Enhanced Student: 0.3896\n",
            "Epoch [9/100], Step [600/600], Loss Enhanced Student: 0.3924\n",
            "Epoch [10/100], Step [600/600], Loss Enhanced Student: 0.4468\n",
            "Epoch [11/100], Step [600/600], Loss Enhanced Student: 0.5708\n",
            "Epoch [12/100], Step [600/600], Loss Enhanced Student: 0.5075\n",
            "Epoch [13/100], Step [600/600], Loss Enhanced Student: 0.4797\n",
            "Epoch [14/100], Step [600/600], Loss Enhanced Student: 0.3302\n",
            "Epoch [15/100], Step [600/600], Loss Enhanced Student: 0.4234\n",
            "Epoch [16/100], Step [600/600], Loss Enhanced Student: 0.3497\n",
            "Epoch [17/100], Step [600/600], Loss Enhanced Student: 0.3489\n",
            "Epoch [18/100], Step [600/600], Loss Enhanced Student: 0.4085\n",
            "Epoch [19/100], Step [600/600], Loss Enhanced Student: 0.3441\n",
            "Epoch [20/100], Step [600/600], Loss Enhanced Student: 0.5019\n",
            "Epoch [21/100], Step [600/600], Loss Enhanced Student: 0.3441\n",
            "Epoch [22/100], Step [600/600], Loss Enhanced Student: 0.3634\n",
            "Epoch [23/100], Step [600/600], Loss Enhanced Student: 0.2983\n",
            "Epoch [24/100], Step [600/600], Loss Enhanced Student: 0.3415\n",
            "Epoch [25/100], Step [600/600], Loss Enhanced Student: 0.3867\n",
            "Epoch [26/100], Step [600/600], Loss Enhanced Student: 0.3885\n",
            "Epoch [27/100], Step [600/600], Loss Enhanced Student: 0.3050\n",
            "Epoch [28/100], Step [600/600], Loss Enhanced Student: 0.3868\n",
            "Epoch [29/100], Step [600/600], Loss Enhanced Student: 0.4067\n",
            "Epoch [30/100], Step [600/600], Loss Enhanced Student: 0.3609\n",
            "Epoch [31/100], Step [600/600], Loss Enhanced Student: 0.3772\n",
            "Epoch [32/100], Step [600/600], Loss Enhanced Student: 0.3374\n",
            "Epoch [33/100], Step [600/600], Loss Enhanced Student: 0.3803\n",
            "Epoch [34/100], Step [600/600], Loss Enhanced Student: 0.3529\n",
            "Epoch [35/100], Step [600/600], Loss Enhanced Student: 0.3243\n",
            "Epoch [36/100], Step [600/600], Loss Enhanced Student: 0.3159\n",
            "Epoch [37/100], Step [600/600], Loss Enhanced Student: 0.3963\n",
            "Epoch [38/100], Step [600/600], Loss Enhanced Student: 0.3911\n",
            "Epoch [39/100], Step [600/600], Loss Enhanced Student: 0.3364\n",
            "Epoch [40/100], Step [600/600], Loss Enhanced Student: 0.2880\n",
            "Epoch [41/100], Step [600/600], Loss Enhanced Student: 0.3254\n",
            "Epoch [42/100], Step [600/600], Loss Enhanced Student: 0.3014\n",
            "Epoch [43/100], Step [600/600], Loss Enhanced Student: 0.3793\n",
            "Epoch [44/100], Step [600/600], Loss Enhanced Student: 0.3523\n",
            "Epoch [45/100], Step [600/600], Loss Enhanced Student: 0.3218\n",
            "Epoch [46/100], Step [600/600], Loss Enhanced Student: 0.4055\n",
            "Epoch [47/100], Step [600/600], Loss Enhanced Student: 0.3071\n",
            "Epoch [48/100], Step [600/600], Loss Enhanced Student: 0.3550\n",
            "Epoch [49/100], Step [600/600], Loss Enhanced Student: 0.3719\n",
            "Epoch [50/100], Step [600/600], Loss Enhanced Student: 0.3821\n",
            "Epoch [51/100], Step [600/600], Loss Enhanced Student: 0.3944\n",
            "Epoch [52/100], Step [600/600], Loss Enhanced Student: 0.3497\n",
            "Epoch [53/100], Step [600/600], Loss Enhanced Student: 0.3080\n",
            "Epoch [54/100], Step [600/600], Loss Enhanced Student: 0.3194\n",
            "Epoch [55/100], Step [600/600], Loss Enhanced Student: 0.2939\n",
            "Epoch [56/100], Step [600/600], Loss Enhanced Student: 0.3395\n",
            "Epoch [57/100], Step [600/600], Loss Enhanced Student: 0.3652\n",
            "Epoch [58/100], Step [600/600], Loss Enhanced Student: 0.2665\n",
            "Epoch [59/100], Step [600/600], Loss Enhanced Student: 0.3363\n",
            "Epoch [60/100], Step [600/600], Loss Enhanced Student: 0.3514\n",
            "Epoch [61/100], Step [600/600], Loss Enhanced Student: 0.3202\n",
            "Epoch [62/100], Step [600/600], Loss Enhanced Student: 0.2960\n",
            "Epoch [63/100], Step [600/600], Loss Enhanced Student: 0.3826\n",
            "Epoch [64/100], Step [600/600], Loss Enhanced Student: 0.3903\n",
            "Epoch [65/100], Step [600/600], Loss Enhanced Student: 0.3259\n",
            "Epoch [66/100], Step [600/600], Loss Enhanced Student: 0.3133\n",
            "Epoch [67/100], Step [600/600], Loss Enhanced Student: 0.4130\n",
            "Epoch [68/100], Step [600/600], Loss Enhanced Student: 0.3347\n",
            "Epoch [69/100], Step [600/600], Loss Enhanced Student: 0.3333\n",
            "Epoch [70/100], Step [600/600], Loss Enhanced Student: 0.3807\n",
            "Epoch [71/100], Step [600/600], Loss Enhanced Student: 0.3679\n",
            "Epoch [72/100], Step [600/600], Loss Enhanced Student: 0.3947\n",
            "Epoch [73/100], Step [600/600], Loss Enhanced Student: 0.2732\n",
            "Epoch [74/100], Step [600/600], Loss Enhanced Student: 0.3407\n",
            "Epoch [75/100], Step [600/600], Loss Enhanced Student: 0.3322\n",
            "Epoch [76/100], Step [600/600], Loss Enhanced Student: 0.3839\n",
            "Epoch [77/100], Step [600/600], Loss Enhanced Student: 0.3371\n",
            "Epoch [78/100], Step [600/600], Loss Enhanced Student: 0.3636\n",
            "Epoch [79/100], Step [600/600], Loss Enhanced Student: 0.2953\n",
            "Epoch [80/100], Step [600/600], Loss Enhanced Student: 0.3056\n",
            "Epoch [81/100], Step [600/600], Loss Enhanced Student: 0.3219\n",
            "Epoch [82/100], Step [600/600], Loss Enhanced Student: 0.3590\n",
            "Epoch [83/100], Step [600/600], Loss Enhanced Student: 0.3515\n",
            "Epoch [84/100], Step [600/600], Loss Enhanced Student: 0.3410\n",
            "Epoch [85/100], Step [600/600], Loss Enhanced Student: 0.3847\n",
            "Epoch [86/100], Step [600/600], Loss Enhanced Student: 0.3608\n",
            "Epoch [87/100], Step [600/600], Loss Enhanced Student: 0.3556\n",
            "Epoch [88/100], Step [600/600], Loss Enhanced Student: 0.3342\n",
            "Epoch [89/100], Step [600/600], Loss Enhanced Student: 0.3247\n",
            "Epoch [90/100], Step [600/600], Loss Enhanced Student: 0.3338\n",
            "Epoch [91/100], Step [600/600], Loss Enhanced Student: 0.3491\n",
            "Epoch [92/100], Step [600/600], Loss Enhanced Student: 0.3126\n",
            "Epoch [93/100], Step [600/600], Loss Enhanced Student: 0.3341\n",
            "Epoch [94/100], Step [600/600], Loss Enhanced Student: 0.2894\n",
            "Epoch [95/100], Step [600/600], Loss Enhanced Student: 0.3193\n",
            "Epoch [96/100], Step [600/600], Loss Enhanced Student: 0.3074\n",
            "Epoch [97/100], Step [600/600], Loss Enhanced Student: 0.3365\n",
            "Epoch [98/100], Step [600/600], Loss Enhanced Student: 0.3445\n",
            "Epoch [99/100], Step [600/600], Loss Enhanced Student: 0.3363\n",
            "Epoch [100/100], Step [600/600], Loss Enhanced Student: 0.3269\n",
            "Accuracy of the network on the 10000 test images: STUDENT ENHANCED: 96.8 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c6AWBw5ooSdS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}