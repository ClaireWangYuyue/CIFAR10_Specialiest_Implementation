{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Group Project -- Final Deliverables\n",
    "**Harvard University**<br>\n",
    "**Fall 2018**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "\n",
    "**Paper Name: Distilling the Knowledge in a Neural Network**<br>\n",
    "**Paper URL: https://arxiv.org/abs/1503.02531**\n",
    "\n",
    "**Team name: ENSEMBLE**<br>\n",
    "**Team members:**<br>\n",
    "-- Timothy Lee (lee709@g.harvard.edu)\n",
    "\n",
    "-- Shiyun Qiu (shiyunqiu@g.harvard.edu)\n",
    "\n",
    "-- Xiangru Shu (xiangru_shu@g.harvard.edu)\n",
    "\n",
    "-- Yuyue Wang (yuyue_wang@g.harvard.edu)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large-scale machine learning tasks, such as speech and object recognition, a cumbersome model is commonly used to acheive high accuracy. The cumbersome model could be an ensemble of smaller models or a single large model trained with a strong regularizer. However, it is impractical to deploy the cumbersome model to a large number of users due to limited computational resources. One solution to this problem is to transfer the knowledge from the cumbersome model to a small model more suitable for deployment through \"distillation\".\n",
    "\n",
    "When we are distilling the knowledge of a cumbersome model into a small model, we should train the small model to generalize in the same way as the cumbersome model so that the small model can generalize well to new data. One way to transfer the generalization ability is to use the class probabilities produced by the cumbersome model as \"soft targets\" when training the small model. When the large model is an ensemble of simpler models, we can take an arithmetic or geometric mean of the predictive distributions as the soft targets. As these soft targets contain probabilities of all incorrect classes in addition to the probability of the correct class, we can obtain a lot of information about how the large model generalizes from the relative probabilites in each training case. Soft targets with high entropy have much less variance in the gradient between training cases, so we can train the small model with less data and a higher learning rate.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distillation\n",
    "\n",
    "#### Class Probability\n",
    "For neural networks, the class probabilities, $q_i$ are usually computed through the softmax output layer,\n",
    "$$ q_i = \\frac{\\exp\\{z_i/T\\}}{\\Sigma_j\\exp\\{z_j/T\\}},$$\n",
    "where $z_i$'s are the logits, i.e. the vector of raw (non-normalized) predictions that a classification model generates, and T is a temperature which is normally set to 1. Higher temperature leads to similar class probabilities (similar $q_i$ values), so it produces a softer probability distribution over classes. \n",
    "\n",
    "#### Simplest Form of Distillation \n",
    "The distilled model will be trained on a transfer set. Each training case in the transfer set is a soft target distribution produced by the cumbersome model with a high temperature in its softmax. We will use the same high temperature to train the distilled model. After the model is trained, we will use a temperature of 1 to predict.\n",
    "\n",
    "#### Correct Labels Partially Known for the Transfer Set\n",
    "Two objective functions are needed.\n",
    "\n",
    "Objective function 1: cross entropy with the soft targets.<br>\n",
    "We need to use the same high temperature used to train the cumbersome model in the softmax of the distilled model. \n",
    "\n",
    "Objective function 2: cross entropy with the correct labels.<br>\n",
    "We need to use the same logits in softmax of the distilled model but at a temperature of 1.\n",
    "\n",
    "$$ \\mathcal{L}(x; W) = \\alpha * \\mathcal{H}(\\sigma(z_c/\\tau), \\sigma(z_d/\\tau)) + \\beta * \\mathcal{H}(y, \\sigma(z_d)), $$\n",
    "where x is the input, W is the model parameters in the distilled model, $\\mathcal{H}$ is the cross entropy function, $\\sigma$ is the softmax function, $\\tau$ is the temperature used to train the cumbersome model, $z_c$ is the logit of the cumbersome model, $z_d$ is the logit of the distilled model, and $\\alpha$ and $\\beta$ are the weights ($\\beta=1-\\alpha$).\n",
    "\n",
    "A weighted average of these two objective functions will be computed with a considerably lower weight placed on the second one (i.e. significantly smaller $\\beta$). \n",
    "\n",
    "The magnitudes of the graidents of soft targets scale as $1/T^2$. To prevent the relative contributions of the hard and soft targets from changing when we adjust meta-parameters, we will multiply both of them by $T^2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Matching as Special Case of Distillation\n",
    "\n",
    "Previous work by Caruana and his collaborators used logits produced by the softmax as the targets for training the small model by minimizing the squared difference between the logits produced by the cumbersome model and those produced by the small model. In this section, we will show that matching the logits is indeed a special case of distillation.\n",
    "\n",
    "The cross-entropy with the soft targets for each class $i$ can be computed as the following,\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mathcal{H}(\\sigma(z_{ci}/\\tau), \\sigma(z_{di}/\\tau)) \n",
    "&= - \\frac{\\exp\\{z_{ci}/\\tau\\}}{\\Sigma_j\\exp\\{z_{cj}/\\tau\\}} * \\log (\\frac{\\exp\\{z_{di}/\\tau\\}}{\\Sigma_j\\exp\\{z_{dj}/\\tau\\}})\\\\\n",
    "&= - \\frac{\\exp\\{z_{ci}/\\tau\\}}{\\Sigma_j\\exp\\{z_{cj}/\\tau\\}} * (z_{di}/\\tau - \n",
    "\\log (\\Sigma_j\\exp\\{z_{dj}/\\tau\\})).\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Taking gradient of the cross-entropy with respect to $z_{di}$ gives us,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{H}(\\sigma(z_{ci}/\\tau), \\sigma(z_{di}/\\tau))}{\\partial z_{di}}\n",
    "&= \\frac{1}{\\tau}(\\frac{\\exp\\{z_{di}/\\tau\\}}{\\Sigma_j\\exp\\{z_{dj}/\\tau\\}} - \\frac{\\exp\\{z_{ci}/\\tau\\}}{\\Sigma_j\\exp\\{z_{cj}/\\tau\\}}).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "When the temperature $\\tau$ is high compared with the magnitude of the logits, we can approximate $\\exp\\{z_{i}/\\tau\\}$ through taylor expansion, i.e. $\\exp\\{z_{i}/\\tau\\} \\approx 1+z_{i}/\\tau$. So the gradient can be estimated as following,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{H}(\\sigma(z_{ci}/\\tau), \\sigma(z_{di}/\\tau))}{\\partial z_{di}}\n",
    "&\\approx \\frac{1}{\\tau}(\\frac{1+z_{di}/\\tau}{N+\\Sigma_j z_{dj}/\\tau} - \\frac{1+z_{ci}/\\tau}{N+\\Sigma_j z_{cj}/\\tau}),\n",
    "\\end{align}\n",
    "$$\n",
    "where N is the number of total classes.\n",
    "\n",
    "When the logits are zero-meaned for each transfer case, i.e. $\\Sigma_j z_{dj} = \\Sigma_j z_{cj} = 0$, the gradient can be simplified to \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\mathcal{H}(\\sigma(z_{ci}/\\tau), \\sigma(z_{di}/\\tau))}{\\partial z_{di}}\n",
    "&\\approx \\frac{1}{\\tau}(\\frac{1+z_{di}/\\tau}{N} - \\frac{1+z_{ci}/\\tau}{N})\\\\\n",
    "&\\approx \\frac{1}{N\\tau^2}(z_{di} - z_{ci}).\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "So when the temperature is high, distillation is also minimizing $\\frac{1}{2}(z_{di} - z_{ci})$, the squared difference between the logits produced by the cumbersome model and those produced by the small model.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Reimplementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "horse   cat truck  frog\n",
      "[1,  2000] loss: 2.222\n",
      "[1,  4000] loss: 1.920\n",
      "[1,  6000] loss: 1.718\n",
      "[1,  8000] loss: 1.628\n",
      "[1, 10000] loss: 1.524\n",
      "[1, 12000] loss: 1.494\n",
      "[2,  2000] loss: 1.393\n",
      "[2,  4000] loss: 1.360\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "outputs = net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
